{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, positive='—Ä–∞–¥–æ—Å—Ç—å', negative = '–≥—Ä—É—Å—Ç—å'):\n",
    "        self.positive = f' {positive.strip()} '\n",
    "        self.negative = f' {negative.strip()} '\n",
    "        \n",
    "        \n",
    "    def load(self):\n",
    "        russian_stopwords = stopwords.words(\"russian\")\n",
    "        russian_stopwords.remove('–Ω–µ')\n",
    "        russian_stopwords.remove('–Ω–µ—Ç')\n",
    "        \n",
    "        self.russian_stopwords = set(russian_stopwords)\n",
    "        self.nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "    def split_hash_tag(self, text):\n",
    "        match = re.search(r\"#([–ê-–Ø–∞-—è]+)\", text)\n",
    "        if match and match.group(1):\n",
    "            replacements = ' '.join(re.findall('[–ê-–Ø][^–ê-–Ø]*', match.group(1)))\n",
    "            return text.replace(match.group(0), replacements)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def remove_parenthesis_pairs(self, text):\n",
    "        text = re.sub(r'((\\(|\\[|\\{)(.*)(\\)|\\]|\\}))', '\\g<2>', text)\n",
    "        return text\n",
    "\n",
    "    def replace_smiles(self, text):    \n",
    "        text = re.sub(r'((:|;|=|8)?(-|%|5|c|—Å)?(\\)|\\]|\\}|3)+|üòú|üòÑ|üòÇ|üíã|‚ô•)', self.positive, text)\n",
    "        text = re.sub(r'((:|;|=|8)(-|%|5|c|—Å)?(d|p|\\*)+)', self.positive, text)\n",
    "\n",
    "        text = re.sub(r'((:|;|=|8)?\\'?(-|%|5|c|—Å)?(\\(|\\[|\\{)+)', self.negative, text)\n",
    "        text = re.sub(r'((:|;|=|8)\\'?(-|%|5|c|—Å)?(g|o)+)', self.negative, text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def replace_obvious_scores(self, text):\n",
    "        text = re.sub(r'([7-9]|1[0-9]) –∏–∑ 10', self.positive, text)\n",
    "        text = re.sub(r'[0-4] –∏–∑ 10', self.negative, text)\n",
    "        return text\n",
    "\n",
    "    def collapse_same_letters(self, text):\n",
    "        text = re.sub(r'([–∞-—è—ë])\\1{2,}', '\\g<1>', text)\n",
    "        return text\n",
    "\n",
    "    def remove_stop_words(self, text):\n",
    "        words = text.split(' ')\n",
    "        text = ' '.join([word for word in words if word not in self.russian_stopwords])\n",
    "        return text\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        text = ' '.join([w.lemma_ for w in self.nlp(text)])\n",
    "        return text\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.split_hash_tag(text)\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('—ë', '–µ')\n",
    "\n",
    "        text = self.remove_parenthesis_pairs(text)\n",
    "        text = self.replace_smiles(text)\n",
    "        text = self.replace_obvious_scores(text)\n",
    "        text = self.collapse_same_letters(text)\n",
    "\n",
    "        text = self.remove_stop_words(text)\n",
    "\n",
    "        text = re.sub(r\"[^–∞-—è ]\", \" \", text)\n",
    "        text = re.sub(r\"[–∞-—è]{35,}\", \"\", text)\n",
    "        text = re.sub(r\" {2,}\", \" \", text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        text = self.lemmatize(text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocessor()\n",
    "text_preprocessor.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–æ—Ç–ª–∏—á–Ω–æ —Å—É–ø–µ—Ä –∫–æ—Å—Ç—é–º —Ä–∞–¥–æ—Å—Ç—å —Å–æ–≤–µ—Ç—É—é'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessor.preprocess('–í—Å—ë –æ—Ç–ª–∏—á–Ω–æ.—Å—É–ø–µ—Ä –∫–æ—Å—Ç—é–º)).—Å–æ–≤–µ—Ç—É—é')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from bpemb import BPEmb\n",
    "from pathlib import Path\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserRequestClassifier:\n",
    "    def __init__(self, neural_net_config_path, neural_net_weights_path,\n",
    "                 input_map_path, input_sequence_config_path, output_map_path,\n",
    "                 embeddings_path):\n",
    "        self.neural_net_config_path = neural_net_config_path\n",
    "        self.neural_net_weights_path = neural_net_weights_path\n",
    "        self.input_map_path = input_map_path\n",
    "        self.input_sequence_config_path = input_sequence_config_path\n",
    "        self.output_map_path = output_map_path\n",
    "        self.embeddings_path = embeddings_path\n",
    "        \n",
    "    def load(self):\n",
    "        with open(self.neural_net_config_path, 'r', encoding='utf-8') as file:\n",
    "            json_model = file.read()\n",
    "\n",
    "        model = tf.keras.models.model_from_json(json_model)\n",
    "        model.load_weights(self.neural_net_weights_path)\n",
    "\n",
    "        with open(self.output_map_path, 'r', encoding='utf-8') as file:\n",
    "            output_map_json = file.read()\n",
    "            \n",
    "        with open(self.input_sequence_config_path, 'r', encoding='utf-8') as file:\n",
    "            self.max_sequence_length = int(file.read())\n",
    "\n",
    "        reversed_output_map = json.loads(output_map_json)\n",
    "        output_map = {v: k for k, v in reversed_output_map.items()}\n",
    "        \n",
    "        self.model = model\n",
    "        self.output_map = output_map\n",
    "        \n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        self.text_preprocessor.load()\n",
    "        \n",
    "        embeddings_keys = []\n",
    "        with open(embeddings_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word = values[0].lower()\n",
    "                embeddings_keys.append(word)\n",
    "        \n",
    "        self.bpemb = BPEmb(lang='ru', cache_dir=Path('./'), dim=100, vs=100000)\n",
    "        \n",
    "        tokenizer = Tokenizer(len(embeddings_keys))\n",
    "        tokenizer.fit_on_texts(embeddings_keys)\n",
    "        \n",
    "        del embeddings_keys[:]        \n",
    "        del embeddings_keys\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def get_vector(self, text):\n",
    "        text = self.text_preprocessor.preprocess(text)\n",
    "        seq = self.tokenizer.texts_to_sequences([self.bpemb.encode(text)])\n",
    "        padded = pad_sequences([seq], self.max_sequence_length)[0]\n",
    "\n",
    "        result = np.array(padded.T)\n",
    "        return result\n",
    "\n",
    "    def classify(self, input_text):\n",
    "        vector = self.model.predict([self.get_vector(input_text)]).T\n",
    "        vector = np.concatenate(vector, axis=0 )\n",
    "        zipped = np.array(list(zip(self.output_map.keys(), vector.T)))\n",
    "        answer = {x[0]:x[1] for x in zipped}\n",
    "        answer = sorted(answer.items(), key=lambda x: float(operator.itemgetter(1)(x)), reverse=True)[:10]\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_map_path = \"model/input-map.json\"\n",
    "input_sequence_config_path = \"model/input-sequence-length.txt\"\n",
    "output_map_path = \"model/output-map.json\"\n",
    "neural_net_config_path = \"model/model-config.json\"\n",
    "neural_net_weights_path = \"model/model-weights.h5\"\n",
    "embeddings_path = './ru/ru.wiki.bpe.vs100000.d100.w2v.txt'\n",
    "\n",
    "classifier = UserRequestClassifier(neural_net_config_path, neural_net_weights_path,\n",
    "                                   input_map_path, input_sequence_config_path,\n",
    "                                   output_map_path, embeddings_path)\n",
    "classifier.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.15936242043972015),\n",
       " (-1.0, 0.14125685393810272),\n",
       " (1.0, 0.045397017151117325)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify('–°–≤–æ–±–æ–¥–∞ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ç–æ–∏—Ç, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–≤–æ–±–æ–¥—É –æ—à–∏–±–∞—Ç—å—Å—è.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division by zero\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    1/0\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
